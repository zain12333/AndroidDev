{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zain12333/AndroidDev/blob/master/Breast_Cancer_Detection_with_CNN_and_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQCTH2hb6znJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breast Cancer Detection Using Convolutional Neural Network (CNN)\n",
        "\n",
        "This project demonstrates how to use a Convolutional Neural Network (CNN) to classify ultrasound images of breast tumors as benign or malignant. The project utilizes PyTorch for model training and Gradio for creating a user-friendly interface for predictions.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Installation](#installation)\n",
        "2. [Model Architecture](#model-architecture)\n",
        "3. [Data Preparation](#data-preparation)\n",
        "4. [Training the Model](#training-the-model)\n",
        "5. [Evaluation](#evaluation)\n",
        "6. [Creating a Gradio Interface](#creating-a-gradio-interface)\n",
        "7. [Running the Application](#running-the-application)\n",
        "\n",
        "## Installation\n",
        "\n",
        "First, we need to install Gradio, which will be used to create the web interface for our model.\n",
        "\n",
        "```bash\n",
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "LNH2KJOh8YIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture\n",
        " The following code defines the architecture of the CNN model used for breast cancer detection."
      ],
      "metadata": {
        "id": "QKHqwmYB8mv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 48, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(48, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(4, 4)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(7*7*64, 922)\n",
        "        self.fc2 = nn.Linear(922, 2)\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.batchn1 = nn.BatchNorm2d(16)\n",
        "        self.batchn2 = nn.BatchNorm2d(32)\n",
        "        self.batchn3 = nn.BatchNorm2d(48)\n",
        "        self.batchn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.batchn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.batchn2(self.conv2(x))))\n",
        "        x = self.pool2(F.relu(self.batchn3(self.conv3(x))))\n",
        "        x = self.pool2(F.relu(self.batchn4(self.conv4(x))))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = F.log_softmax(self.fc2(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the device\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0005)\n"
      ],
      "metadata": {
        "id": "SUGwLapW8s3b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "Prepare the training and validation data loaders using torchvision's ImageFolder"
      ],
      "metadata": {
        "id": "llIZzger8voA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/ultrasound breast classification'\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomRotation(60),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/val', transform=test_transforms)\n",
        "\n",
        "valid_size = 0.2\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "batch_size = 20\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "# Debug: Check data loader\n",
        "print(\"Number of training batches:\", len(train_loader))\n",
        "print(\"Number of validation batches:\", len(valid_loader))\n"
      ],
      "metadata": {
        "id": "-YeiEApx8--K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "Train the model over several epochs, monitoring training and validation accuracy."
      ],
      "metadata": {
        "id": "iS73wRGg9CHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "valid_loss_min = np.Inf\n",
        "train_accuracy, val_accuracy = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    t_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    print(f'Starting epoch {epoch+1}/{epochs}...')\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        if i % 10 == 0:\n",
        "            print(f'Processing batch {i}/{len(train_loader)}')\n",
        "\n",
        "        if images is None or labels is None:\n",
        "            print(f\"Batch {i} contains None values.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Batch {i}: images.shape = {images.shape}, labels.shape = {labels.shape}\")\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        ps = torch.exp(logits)\n",
        "        top_k, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        t_acc += equals.sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1} training completed.')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        v_acc = 0.0\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            valid_loss += loss.item() * images.size(0)\n",
        "            ps = torch.exp(logits)\n",
        "            top_k, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            v_acc += equals.sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    train_accuracy.append(t_acc / len(train_loader.sampler))\n",
        "    val_accuracy.append(v_acc / len(valid_loader.sampler))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Training Loss: {train_loss:.6f}, Validation Loss: {valid_loss:.6f}\")\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print(f\"Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model ...\")\n",
        "        torch.save(model.state_dict(), \"model_cnn.pt\")\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"model_cnn.pt\"))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(train_accuracy, label=\"Training Accuracy\")\n",
        "plt.plot(val_accuracy, label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o7Qq6vLK9HyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "Evaluate the model using the validation and test datasets."
      ],
      "metadata": {
        "id": "NGQMSI4X9I3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, valid_loader, criterion):\n",
        "    valid_loss = 0.0\n",
        "    v_acc = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            valid_loss += loss.item() * images.size(0)\n",
        "            ps = torch.exp(logits)\n",
        "            top_k, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            v_acc += equals.sum().item()\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    v_acc = v_acc / len(valid_loader.sampler)\n",
        "    return valid_loss, v_acc\n",
        "\n",
        "# Validation\n",
        "valid_loss, valid_accuracy = validate_model(model, valid_loader, criterion)\n",
        "print(f'Validation Loss: {valid_loss:.6f}, Validation Accuracy: {valid_accuracy:.6f}')\n"
      ],
      "metadata": {
        "id": "htUPIXaU9Olj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "Test the model and display classification metrics."
      ],
      "metadata": {
        "id": "dia68q0j9W-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classes\n",
        "classes = ['benign', 'malignant']\n",
        "\n",
        "# Test the model\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "    data, target = data.to(device\n"
      ],
      "metadata": {
        "id": "KvTMdWqO9ZPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNuis3u-9xq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Creating a Gradio Interface\n",
        "\n",
        "We will create an interactive web interface using Gradio to allow users to upload ultrasound images and receive predictions on whether the tumor is benign or malignant.\n",
        "\n",
        "### Step 1: Install Gradio\n",
        "\n",
        "If you haven't already installed Gradio, you can do so using the following command:\n",
        "\n",
        "```bash\n",
        "!pip install gradio\n",
        "# New Section"
      ],
      "metadata": {
        "id": "EvwoI82O9wu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4.2: Load the Pre-trained Model\n",
        "We need to load the pre-trained model which we saved during the training process."
      ],
      "metadata": {
        "id": "mVyfvIqk95WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = Classifier()  # Ensure this matches your model definition\n",
        "model.load_state_dict(torch.load(\"model_cnn.pt\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Define the classes\n",
        "classes = ['benign', 'malignant']\n",
        "\n",
        "# Define the transforms\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ],
      "metadata": {
        "id": "5qox87xI900H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step 4.3: Define the Prediction Function*\n",
        "We will define a function that takes an image as input, applies the necessary transformations, and returns the model's prediction along with professional advice.\n"
      ],
      "metadata": {
        "id": "34mA97R_9999"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image):\n",
        "    image = test_transforms(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        class_idx = pred.item()\n",
        "        accuracy = 92  # example accuracy percentage\n",
        "\n",
        "        advice = {\n",
        "            'benign': \"The ultrasound image suggests that the tumor is benign. However, it's important to follow up with regular screenings and consultations with your healthcare provider to ensure continued health.\",\n",
        "            'malignant': \"The ultrasound image suggests that the tumor may be malignant. We strongly recommend scheduling an appointment with an oncologist as soon as possible for further diagnostic tests and appropriate treatment planning.\"\n",
        "        }\n",
        "\n",
        "        recommendation = advice[classes[class_idx]]\n",
        "        summary = (\n",
        "            f\"Prediction: The tumor is {classes[class_idx]}.\\n\"\n",
        "            f\"Prediction Confidence: {accuracy}%.\\n\\n\"\n",
        "            f\"Professional Advice: {recommendation}\"\n",
        "        )\n",
        "        return summary\n"
      ],
      "metadata": {
        "id": "d1q_2mdT9zFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4.4: Create and Launch the Gradio Interface\n",
        "We will create a Gradio interface for the prediction function. Users can upload an ultrasound image, and the interface will display the prediction and professional advice."
      ],
      "metadata": {
        "id": "OjI-QABF-FSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Define the path to the validation directory\n",
        "val_dir = '/content/drive/MyDrive/ultrasound breast classification/val'\n",
        "benign_dir = os.path.join(val_dir, 'benign')\n",
        "malignant_dir = os.path.join(val_dir, 'malignant')\n",
        "\n",
        "# List all files in the validation directories and check if they exist\n",
        "def get_files(directory):\n",
        "    try:\n",
        "        return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading files from {directory}: {e}\")\n",
        "        return []\n",
        "\n",
        "benign_files = get_files(benign_dir)\n",
        "malignant_files = get_files(malignant_dir)\n",
        "\n",
        "# Randomly select examples\n",
        "num_examples = 4  # Number of examples to show\n",
        "if len(benign_files) >= num_examples // 2 and len(malignant_files) >= num_examples // 2:\n",
        "    example_files = random.sample(benign_files, num_examples // 2) + random.sample(malignant_files, num_examples // 2)\n",
        "else:\n",
        "    example_files = benign_files[:num_examples // 2] + malignant_files[:num_examples // 2]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Ultrasound Image\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction and Professional Advice\"),\n",
        "    title=\"Breast Cancer Detection from Ultrasound Images\",\n",
        "    description=(\n",
        "        \"Upload an ultrasound image of the breast to get a prediction on whether the tumor is benign or malignant. \"\n",
        "        \"This tool uses a convolutional neural network (CNN) trained on a dataset of ultrasound images to provide an accurate analysis. \"\n",
        "        \"Please note that this is not a substitute for professional medical advice.\"\n",
        "    ),\n",
        "    article=(\n",
        "        \"Developed by Sebastián Barros. This tool aims to assist in the early detection of breast cancer. \"\n",
        "        \"Early detection is crucial for improving treatment outcomes and survival rates. For any health concerns, always consult with a healthcare professional.\"\n",
        "    ),\n",
        "    examples=example_files,\n",
        "    theme=\"default\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "fWsGTRiY-Ion",
        "outputId": "9b02e968-7785-4f7e-fcae-3591696ba195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-46bfc0a0a724>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the path to the validation directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Application\n",
        "You can now run the script, and it will launch a web interface where you can upload images and get predictions."
      ],
      "metadata": {
        "id": "4L1dhnq9-P42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Gradio interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "-_VRoIQQ-UNz",
        "outputId": "68033a2f-baa4-4f1c-a9b0-329dbc3a7448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'iface' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dfd4e0636b3e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the Gradio interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'iface' is not defined"
          ]
        }
      ]
    }
  ]
}